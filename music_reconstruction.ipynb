{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bealowman/music-reconstruction-working/blob/drive-data-loader/music_reconstruction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pipeline"
      ],
      "metadata": {
        "id": "xk7RDlhFeDmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CWlK9GnItesb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "685477e5-06db-498e-ec57-2a130d1df9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['artifacts', 'dataInfo', 'ecog']\n",
            "['artifacts', 'dataInfo', 'ecog']\n"
          ]
        }
      ],
      "source": [
        "## DATA LOADING LOCAL ##\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "hfa = np.load('HFA.npy')\n",
        "\n",
        "spectrogram = np.load('spectrogram.npy')\n",
        "\n",
        "print(hfa.shape, spectrogram.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DATA LOADING FROM DRIVE (PREFERRED): INSPECT KEYS ##\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "\n",
        "base_directory = \"/content/drive/MyDrive/Neurotech Music Reconstruction Data/Zenodo\"\n",
        "id = 1 # change from 1 to 29 as needed\n",
        "\n",
        "hfa_path = os.path.join(base_directory, f\"P{id}_HFA_data.mat\")\n",
        "hfa_mat = loadmat(hfa_path, squeeze_me=True, struct_as_record=False)\n",
        "\n",
        "hfa_keys = [k for k in hfa_mat.keys() if not k.startswith(\"__\")]\n",
        "print(\"HFA keys:\", hfa_keys)\n",
        "\n",
        "\n",
        "stim_path = os.path.join(base_directory, \"thewall1_stim32.mat\")\n",
        "stim_mat = loadmat(stim_path, squeeze_me=True, struct_as_record=False)\n",
        "\n",
        "stim_keys = [k for k in stim_mat.keys() if not k.startswith(\"__\")]\n",
        "print(\"STIM keys:\", stim_keys)\n",
        "\n",
        "#spectrogram = loadmat(stim_path)['stim'].astype(np.float32)\n",
        "\n",
        "#if spectrogram.shape[0] < spectrogram.shape[1]:\n",
        "#  spectrogram = spectrogram.T\n",
        "\n",
        "#print(\"HFA:\", hfa.shape, \"Spectrogram:\", spectrogram.shape)"
      ],
      "metadata": {
        "id": "gYnEXoxk8WVC",
        "outputId": "ed3afb4f-b5ac-474b-8be8-ab555116dd40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "HFA keys: ['artifacts', 'dataInfo', 'ecog']\n",
            "STIM keys: ['params', 'stim32', 'CF32']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## DATA LOADING FROM DRIVE (PREFERRED) ##\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "\n",
        "base_directory = \"/content/drive/MyDrive/Neurotech Music Reconstruction Data/Zenodo\"\n",
        "id = 1 # change from 1 to 29 as needed\n",
        "\n",
        "hfa_path = os.path.join(base_directory, f\"P{id}_HFA_data.mat\")\n",
        "hfa_mat = loadmat(hfa_path, squeeze_me=True, struct_as_record=False)\n",
        "ecog = hfa_mat[\"ecog\"].item()\n",
        "hfa = np.asarray(ecog.HFA, dtype=np.float32)\n",
        "\n",
        "if hfa.shape[0] < hfa.shape[1]:\n",
        "  hfa = hfa.T"
      ],
      "metadata": {
        "id": "ns0Hm67d_QcJ",
        "outputId": "e6c96592-db05-49a9-b942-02e86cb23b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "can only convert an array of size 1 to a Python scalar",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2322829552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mhfa_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"P{id}_HFA_data.mat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mhfa_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhfa_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze_me\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstruct_as_record\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mecog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhfa_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ecog\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mhfa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mecog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHFA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_lagged_spectrogram(spectrogram, n_lags):\n",
        "    lagged_features, targets = [], []\n",
        "    for t in range(n_lags, len(spectrogram)):\n",
        "        feat = spectrogram[t-n_lags:t].flatten()\n",
        "        lagged_features.append(feat)\n",
        "        targets.append(hfa[t])\n",
        "    return np.array(lagged_features), np.array(targets)\n",
        "\n",
        "n_lags = 75\n",
        "X_lagged, y = create_lagged_spectrogram(spectrogram, n_lags)\n",
        "print(X_lagged.shape, y.shape)\n",
        "\n",
        "#train here\n",
        "\n",
        "#Fit Linear Regression Encoding Model\n",
        "\n",
        "#Visualize STRF\n",
        "\n",
        "#\n"
      ],
      "metadata": {
        "id": "2JDGjfRtPi90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.processing import RobustScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import Huber\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.stats import pearsonr\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.stats import zscore\n",
        "\n",
        "all_r = []\n",
        "all_r2 = []\n",
        "all_coeffs = []\n",
        "\n",
        "# iterate 250 times\n",
        "iterations = 250\n",
        "\n",
        "n_samples, n_features = X_lagged.shape\n",
        "\n",
        "# \"We defined relatively long, 2-second groups of\n",
        "#  consecutive samples as indivisible blocks of data\"\n",
        "# sampling rate of 100Hz, 2 seconds = 200 samples\n",
        "group_size = 200\n",
        "groups = np.arange(n_samples) // group_size\n",
        "\n",
        "# 60% train. 20% validation, 20% test\n",
        "# Assign 60% train, 40% temp (test/val)\n",
        "training = GroupShuffleSplit(n_splits=iterations, test_size=0.6, random_state=42)\n",
        "\n",
        "# 50% test, 50% validation (from 40% temp)\n",
        "val_test_temp = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
        "\n",
        "for i, (train_index, temp_index) in enumerate(training.split(X_lagged, y, groups)):\n",
        "\n",
        "    # Split temp into test and val\n",
        "    temp_groups = groups[temp_index]\n",
        "    relative_val_index, relative_test_index = next(val_test_temp.split(X_lagged[temp_index], y[temp_index], temp_groups))\n",
        "    val_index = temp_index[relative_val_index]\n",
        "    test_index = temp_index[relative_test_index]\n",
        "\n",
        "    X_train, y_train = X_lagged[train_index], y[train_index]\n",
        "    X_val, y_val = X_lagged[val_index], y[val_index]\n",
        "    X_test, y_test = X_lagged[test_index], y[test_index]\n",
        "\n",
        "    # Standardization\n",
        "    # \"standardized the features by fitting a robust scaler to the\n",
        "    #  training set only (estimates the median and the 2 to 98 quantile range)\"\n",
        "    scaler = RobustScaler(quantile_range=(2, 98))\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Model (TensorFlow)\n",
        "    model = Sequential([Dense(1, input_shape=(n_features,), activation='linear')])\n",
        "\n",
        "    # RMSProp optimizer\n",
        "    optimizer = RMSprop(learning_rate=0.001)\n",
        "\n",
        "    # Huber loss function\n",
        "    loss = Huber()\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    # \"early stopping to further prevent overfitting\"\n",
        "    # \"estimated on the validation set at each training step, and model\n",
        "    #  fitting ends after this error stops diminishing for 10 consecutive steps\"\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10,\n",
        "                                   restore_best_weights=True)\n",
        "\n",
        "    # Model fitting\n",
        "    model.fit(X_train_scaled, y_train, validation_data=(X_val_scaled, y_val),\n",
        "              epochs=150, batch_size=64, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "    # Evaluation\n",
        "    y_pred = model.predict(X_test_scaled).flatten()\n",
        "\n",
        "    # Correlation Coefficient\n",
        "    r = pearsonr(y_test, y_pred)[0]\n",
        "\n",
        "    # r squared\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    # Model's coefficients\n",
        "    coeffs = model.layers[0].get_weights()[0].flatten()\n",
        "\n",
        "    all_r.append(r)\n",
        "    all_r2.append(r2)\n",
        "    all_coeffs.append(coeffs)\n",
        "\n",
        "all_r = np.array(all_r)\n",
        "all_r2 = np.array(all_r2)\n",
        "all_coeffs = np.array(all_coeffs)\n",
        "\n",
        "# \"z-scored each coefficient across the 250 models\"\n",
        "z_scores = zscore(all_coeffs, axis=0)\n",
        "final_strf = np.mean(all_coeffs, axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "FRCQV56-cCIP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}